input {
  # Query => utofeed mode enables tracking of downloaded records on the server-side since the
  # http_poller input plugin has no logic for importing changes.
  # Uncomment the query => confidential line to include confidential records.
  # Uncomment the query => release_status line to include unreleased records.
  http_poller {
    urls => {
      indicia => {
        method => get
        url => "{{ Warehouse URL }}/index.php/services/rest/reports/library/occurrences/list_for_elastic.xml"
        headers => {
          Accept => "application/json"
          Authorization => "USER:{{ User }}:SECRET:{{ Secret }}"
        }
        query => {
          proj_id => "{{ Project ID }}"
          from_update_date => "1900-01-01"
          autofeed => "t"
          # confidential => "all"
          # release_status => "A"
        }
      }
    }
    request_timeout => 120
    connect_timeout => 30
    socket_timeout => 60
    schedule => { cron => "*/2 * * * * UTC" }
    keepalive => false
    codec => "json"
  }
}
filter {
  # Lookup taxon information from our YAML File and store in the
  # taxon_data_blob field. Translation can only read one field from the
  # YAML so we store all the data in a single field separated by ~.
  translate {
    dictionary_path => "{{ Working folder path }}/data/taxa.yml"
    field => "taxon_key"
    destination => "taxon_data_blob"
  }
  # Split all the taxon data blob field content into separate fields.
  csv {
    source => "taxon_data_blob"
    separator => "~"
    skip_empty_columns => true
    quote_char => "`"
    columns => [
      "taxon_name",
      "taxon_authority",
      "accepted_taxon_key",
      "accepted_taxon_name",
      "accepted_taxon_authority",
      "taxon_group",
      "common_name",
      "taxon_rank",
      "taxon_rank_sort_order",
      "marine_flag",
      "taxon_kingdom",
      "taxon_phylum",
      "taxon_class",
      "taxon_order",
      "taxon_family",
      "taxon_subfamily",
      "taxon_genus",
      "taxon_species",
      "taxon_species_key"
    ]
    remove_field => "taxon_data_blob"
  }
  # Lookup the taxon path data and attach to the document.
  translate {
    dictionary_path => "{{ Working folder path }}/data/taxon-paths.yml"
    field => "accepted_taxon_key"
    destination => "taxon_path_keys"
  }
  # Split the taxon path keys into separate items.
  mutate {
    split => {
      "taxon_path_keys" => ","
    }
  }
  # Split the indexed location IDs into separate items.
  mutate {
    split => {
      "indexed_location_ids" => ","
    }
  }
  # Map the indexed locations keys list into a blob of location information.
  translate {
    dictionary_path => "/Users/john/Documents/Git repos/indicia_support_files/Elasticsearch/data/locations.yml"
    field => "indexed_location_ids"
    iterate_on => "indexed_location_ids"
    destination => "locations_blob"
    remove_field => "indexed_location_ids"
  }
  # Split the indexed locations blob into a list of structured location objects.
  ruby {
    code => "
      if event.get('locations_blob')
        event.set('locations', event.get('locations_blob').collect { |t|
          c = t.split '~'
          {
            'id' => c[0],
            'name' =>  c[1],
            'code' => c[2],
            'type' => c[3]
          }
        })
      end
    "
    remove_field => "locations_blob"
  }
  # Split the media into separate items.
  mutate {
    split => {
      "media" => ","
    }
  }
  # Tidy data cleaner info. First, add a pipe character between items in the
  # text so it can be split easily.
  mutate {
    gsub => [
      "data_cleaner_info", "\} \[", "}|["
    ]
  }
  # Now, split the data cleaner info into separate items.
  mutate {
    split => {
      "data_cleaner_info" => "|"
    }
  }
  ruby {
    code => "
      if event.get('data_cleaner_info')
        event.set('data_cleaner_list', event.get('data_cleaner_info').reject{|t| t == 'pass'}.collect { |t|
          t.tr!('[}', '')
          c = t.split ']{'
          {
            'rule_type' =>  c[0].sub('data_cleaner_', '').split('_').collect(&:capitalize).join,
            'message' => c[1]
          }
        })
      end
    "
    remove_field => "data_cleaner_info"
    add_field => { "warehouse" => "{{ Indicia warehouse unique name }}" }
  }
  # Convert our list of fields into a nicely structured occurrence document.
  mutate {
    rename => {
      "created_on" => "[metadata][created_on]"
      "updated_on" => "[metadata][updated_on]"
      "created_by_id" => "[metadata][created_by_id]"
      "updated_by_id" => "[metadata][updated_by_id]"
      "licence_code" => "[metadata][licence_code]"
      "group_id" => "[metadata][group][id]"
      "group_title" => "[metadata][group][title]"
      "survey_id" => "[metadata][survey][id]"
      "survey_title" => "[metadata][survey][title]"
      "website_id" => "[metadata][website][id]"
      "website_title" => "[metadata][website][title]"
      "sensitive" => "[metadata][sensitive]"
      "sensitivity_precision" => "[metadata][sensitivity_blur]"
      "sensitivity_blur" => "[metadata][sensitivity_blur]"
      "trial" => "[metadata][trial]"
      "confidential" => "[metadata][confidential]"
      "release_status" => "[metadata][release_status]"
      "attr_det_name" => "[identification][identified_by]"
      "verifier" => "[identification][verifier][name]"
      "verified_by_id" => "[identification][verifier][id]"
      "verified_on" => "[identification][verified_on]"
      "record_status" => "[identification][identification_verification_status]"
      "data_cleaner_list" => "[identification][auto_checks][output]"
      "data_cleaner_result" => "[identification][auto_checks][result]"
      "verification_checks_enabled" => "[identification][auto_checks][enabled]"
      "query" => "[identification][query]"
      "attr_certainty" => "[identification][recorder_certainty]"
      "point" => "[location][point]"
      "geom" => "[location][geom]"
      "output_sref" => "[location][output_sref]"
      "output_sref_system" => "[location][output_sref_system]"
      "coordinate_uncertainty_in_meters" => "[location][coordinate_uncertainty_in_meters]"
      "given_locality_name" => "[location][verbatim_locality]"
      "recorded_location_id" => "[location][location_id]"
      "recorded_location_name" => "[location][name]"
      "recorded_parent_location_id" => "[location][parent][location_id]"
      "recorded_parent_location_name" => "[location][parent][name]"
      "locations" => "[location][higher_geography]"
      "sample_id" => "[event][event_id]"
      "parent_sample_id" => "[event][parent_event_id]"
      "date_start" => "[event][date_start]"
      "date_end" => "[event][date_end]"
      "day_of_year" => "[event][day_of_year]"
      "week" => "[event][week]"
      "ukbms_week" => "[event][ukbms_week]"
      "month" => "[event][month]"
      "year" => "[event][year]"
      "recorders" => "[event][recorded_by]"
      "sample_comment" => "[event][event_remarks]"
      "attr_biotope" => "[event][habitat]"
      "attr_sample_method" => "[event][sampling_protocol]"
      "taxon_key" => "[taxon][taxon_id]"
      "taxon_name" => "[taxon][taxon_name]"
      "taxon_authority" => "[taxon][taxon_name_authorship]"
      "accepted_taxon_key" => "[taxon][accepted_taxon_id]"
      "accepted_taxon_name" => "[taxon][accepted_name]"
      "accepted_taxon_authority" => "[taxon][accepted_name_authorship]"
      "taxon_group" => "[taxon][group]"
      "common_name" => "[taxon][vernacular_name]"
      "taxon_rank" => "[taxon][taxon_rank]"
      "taxon_rank_sort_order" => "[taxon][taxon_rank_sort_order]"
      "marine_flag" => "[taxon][marine]"
      "taxon_kingdom" => "[taxon][kingdom]"
      "taxon_phylum" => "[taxon][phylum]"
      "taxon_class" => "[taxon][class]"
      "taxon_order" => "[taxon][order]"
      "taxon_family" => "[taxon][family]"
      "taxon_subfamily" => "[taxon][subfamily]"
      "taxon_genus" => "[taxon][genus]"
      "taxon_species" => "[taxon][species]"
      "taxon_species_key" => "[taxon][species_taxon_id]"
      "taxon_path_keys" => "[taxon][higher_taxon_ids]"
      "attr_sex" => "[occurrence][sex]"
      "attr_stage" => "[occurrence][life_stage]"
      "attr_sex_stage_count" => "[occurrence][organism_quantity]"
      "attr_sex_stage_count_exact" => "[occurrence][individual_count]"
      "zero_abundance" => "[occurrence][zero_abundance]"
      "comment" => "[occurrence][occurrence_remarks]"
      "media" => "[occurrence][associated_media]"
    }
  }
  ruby {
    # Script to clean up nulls and empty values.
    path => "{{ Working folder path }}/logstash-rb/compact_event.rb"
  }
  # An http error or similar will not have a document ID, so create a uuid to
  # keep the document ID unique. Otherwise we only log 1 error event.
  if "" in [id] {
    uuid {
      target => "id"
    }
  }
}
output {
  elasticsearch {
    hosts => ["{{ Elasticsearch address }}"]
    index => "occurrence_{{ Indicia warehouse unique name }}_index"
    # Our records need a unique ID in the index for updates.
    document_id => "{{ Indicia warehouse unique name }}|%{id}"
  }
  # Uncomment the following to see the data output to the terminal.
  # stdout { codec => json }
}