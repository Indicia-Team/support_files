input {
    jdbc {
        # Postgres jdbc connection string to our database, mydb
        jdbc_connection_string => "jdbc:postgresql://{{ DB path }}"
        # The user we wish to execute our statement as
        jdbc_user => "{{ DB user }}"
        jdbc_password => "{{ DB password }}"
        # The path to our downloaded jdbc driver
        jdbc_driver_library => "{{ JDBC jar file path }}"
        # The name of the driver class for Postgresql
        jdbc_driver_class => "org.postgresql.Driver"
        # Uncomment the following to rebuild from scratch
        # clean_run => true
        # our query
        statement => "
        select o.id,
          o.external_key,
          o.created_by_id,
          o.survey_id,
          o.website_id,
          o.group_id,
          snf.website_title,
          snf.survey_title,
          snf.group_title,
          snf.recorders,
          o.taxa_taxon_list_external_key as taxon_key,
          o.date_start,
          o.date_end,
          o.date_type,
          case o.date_type when 'D' then extract(doy from o.date_start) else null end as day_of_year,
          case
            when coalesce(date_part('year', o.date_start), 0)=coalesce(date_part('year', o.date_end), 0)
              and coalesce(floor(extract(doy from o.date_start)/7+1), 0)=coalesce(floor(extract(doy from date_end)/7+1), 0) then coalesce(floor(extract(doy from o.date_start)/7+1), 0)
            else null
          end as week,
          case
            when coalesce(date_part('year', o.date_start), 0)=coalesce(date_part('year', o.date_end), 0)
                and
                  floor((extract('doy' from o.date_start) - extract('doy' from (extract('year' from o.date_start) || '-04-01')::date))/7)
                  =
                  floor((extract('doy' from o.date_end) - extract('doy' from (extract('year' from o.date_end) || '-04-01')::date))/7)
              then floor((extract('doy' from o.date_start) - extract('doy' from (extract('year' from o.date_start) || '-04-01')::date))/7) + 1
            else null
          end as ukbms_week,
          case
            when coalesce(date_part('year', o.date_start), 0)=coalesce(date_part('year', o.date_end), 0)
              and coalesce(date_part('month', o.date_start), 0)=coalesce(date_part('month', o.date_end), 0) then coalesce(date_part('month', o.date_start), 0)
            else null
          end as month,
          case
            when coalesce(date_part('year', o.date_start), 0)=coalesce(date_part('year', o.date_end), 0) then coalesce(date_part('year', o.date_start), null)
            else null
          end as year,
          st_astext(st_removerepeatedpoints(st_transform(o.public_geom, 4326))) as geom,
          st_x(st_transform(st_centroid(o.public_geom), 4326)) as point_x,
          st_y(st_transform(st_centroid(o.public_geom), 4326)) as point_y,
          get_output_system(o.public_geom, '3857') as map_sq_srid,
          TRIM(leading 'POINT(' FROM TRIM(trailing ')' FROM st_astext(st_centroid(st_transform(reduce_precision(o.public_geom, false, 1000, '4326'), 4326))))) as map_sq_1km,
          TRIM(leading 'POINT(' FROM TRIM(trailing ')' FROM st_astext(st_centroid(st_transform(reduce_precision(o.public_geom, false, 2000, '4326'), 4326))))) as map_sq_2km,
          TRIM(leading 'POINT(' FROM TRIM(trailing ')' FROM st_astext(st_centroid(st_transform(reduce_precision(o.public_geom, false, 10000, '4326'), 4326))))) as map_sq_10km,
          snf.public_entered_sref as input_sref,
          snf.entered_sref_system as input_sref_system,
          onf.output_sref,
          onf.output_sref_system,
          CASE
            WHEN snf.entered_sref_system NOT SIMILAR TO '[0-9]+' THEN
              get_sref_precision(onf.output_sref, onf.output_sref_system, null)
            ELSE COALESCE(snf.attr_sref_precision, 50)
          END as coordinate_uncertainty_in_meters,
          o.location_name as given_locality_name,
          l.id as recorded_location_id,
          l.name as recorded_location_name,
          lp.id as recorded_parent_location_id,
          lp.name as recorded_parent_location_name
          o.location_ids,
          onf.comment,
          snf.comment as sample_comment,
          onf.licence_code,
          coalesce(onf.attr_stage, onf.attr_sex_stage) as attr_stage,
          onf.attr_sex,
          onf.attr_sex_stage_count,
          case when onf.attr_sex_stage_count similar to '\d{1,9}' then onf.attr_sex_stage_count::integer else null end as attr_sex_stage_count_exact,
          onf.attr_certainty,
          coalesce(onf.attr_det_full_name, coalesce(onf.attr_det_first_name, '') || ' ' || onf.attr_det_last_name) as attr_det_name,
          snf.attr_biotope,
          snf.attr_sample_method,
          snf.media,
          cttl.created_on,
          greatest(o.updated_on, cttl.cache_updated_on) as updated_on,
          o.record_status
          o.record_substatus,
          occ.record_decision_source,
          occ.verified_by_id,
          onf.verifier,
          o.verified_on,
          onf.data_cleaner_info,
          o.query,
          o.sensitive,
          onf.sensitivity_precision,
          CASE o.sensitive WHEN true THEN 'B' ELSE null END AS sensitivity_blur,
          o.confidential,
          o.release_status,
          onf.attrs_json,
          snf.attrs_json,
          o.training,
          o.tracking,
          o.input_form,
          CASE
          WHEN o.media_count>0 AND o.sensitive=false AND snf.privacy_precision IS NULL THEN
            (SELECT string_agg(om.path || '~~' || COALESCE(om.caption, '') || '~~' ||  COALESCE(l.code, '') || '~~' ||  COALESCE(t.term, ''), '@@')
            FROM occurrence_media om
            LEFT JOIN licences l ON l.id=om.licence_id
            LEFT JOIN cache_termlists_terms t ON t.id=om.media_type_id
            WHERE om.occurrence_id=o.id AND om.deleted=false)
          ELSE null
          END as media_data,
          cttl.taxon_list_id,
          cttl.taxon_list_title
        from cache_occurrences_functional o
        join cache_occurrences_nonfunctional onf on onf.id=o.id
        join occurrences occ on occ.id=o.id and occ.deleted=false
        join cache_samples_nonfunctional snf on snf.id=o.sample_id
        join cache_taxa_taxon_lists cttl on cttl.id=o.taxa_taxon_list_id
        left join locations l on l.id=o.location_id and l.deleted=false
        left join samples sp on sp.id=o.parent_sample_id and sp.deleted=false
        left join locations lp on lp.id=sp.location_id and lp.deleted=false
        where o.tracking > :sql_last_value
        and o.training=false
        -- comment out the following line to include confidential records in the index
        and o.confidential=false
        -- comment out the following line to include unreleased records in the index
        and o.release_status='R'
        and o.zero_abundance=false"
        tracking_column => "tracking"
        tracking_column_type => "integer"
    }
}
filter {
  # Lookup taxon information from our CSV File and store in the
  # taxon_data_csv field. Translation can only read one field from the
  # CSV so we store all the data in a single field separated by ~.
  translate {
    dictionary_path => "{{ Working folder path }}/data/taxa.csv"
    field => "taxon_key"
    destination => "taxon_data_csv"
  }
  # Split all the taxon data csv field content into separate fields.
  csv {
    source => "taxon_data_csv"
    separator => "~"
    skip_empty_columns => true
    quote_char => "`"
    columns => [
      "taxon_name",
      "taxon_authority",
      "accepted_taxon_key",
      "accepted_taxon_name",
      "accepted_taxon_authority",
      "taxon_group_id",
      "taxon_group",
      "common_name",
      "taxon_rank",
      "taxon_rank_sort_order",
      "marine_flag",
      "freshwater_flag",
      "terrestrial_flag",
      "non_native_flag",
      "taxon_family",
      "taxon_subfamily",
      "taxon_genus",
      "taxon_species",
      "taxon_species_key"
    ]
    remove_field => "taxon_data_csv"
  }
  # Lookup the taxon path data and attach to the document.
  translate {
    dictionary_path => "{{ Working folder path }}/data/taxon-paths.csv"
    field => "accepted_taxon_key"
    destination => "taxon_path_keys"
  }
  # Split the taxon path keys into separate items.
  mutate {
    split => {
      "taxon_path_keys" => ","
    }
  }
  # Split the indexed location IDs into separate items.
  mutate {
    split => {
      "indexed_location_ids" => ","
    }
  }
  # Map the indexed locations keys list into a blob of location information.
  translate {
    dictionary_path => "{{ Working folder path }}/data/locations.yml"
    field => "indexed_location_ids"
    iterate_on => "indexed_location_ids"
    destination => "locations_blob"
    remove_field => "indexed_location_ids"
  }
  # Split the indexed locations blob into a list of structured location objects.
  ruby {
    code => "
      if event.get('locations_blob')
        event.set('locations', event.get('locations_blob').compact.collect { |t|
          c = t.split '~'
          {
            'id' => c[0],
            'name' =>  c[1],
            'code' => c[2],
            'type' => c[3]
          }
        })
      end
    "
    remove_field => "locations_blob"
  }
  # Split the media into separate items.
  mutate {
    split => {
      "media_data" => "@@"
    }
  }
  # Convert the media data for each item into an object.
  ruby {
    code => "
      if event.get('media_data')
        event.set('media_list', event.get('media_data').collect { |t|
          c = t.split '~~'
          {
            'path' =>  c[0],
            'caption' => c[1],
            'licence' => c[2],
            'type' => c[3]
          }.delete_if { |k, v| v.blank? }
        })
      end
    "
    remove_field => "media_data"
  }
  # Tidy data cleaner info. First, add a pipe character between items in the
  # text so it can be split easily.
  mutate {
    gsub => [
      "data_cleaner_info", "\} \[", "}|["
    ]
  }
  # Now, split the data cleaner info into separate items.
  mutate {
    split => {
      "data_cleaner_info" => "|"
    }
  }
  ruby {
    code => "
      if event.get('data_cleaner_info')
        event.set('data_cleaner_list', event.get('data_cleaner_info').reject{|t| t == 'pass'}.collect { |t|
          t.tr!('[}', '')
          c = t.split ']{'
          {
            'rule_type' =>  c[0].sub('data_cleaner_', '').split('_').collect(&:capitalize).join,
            'message' => c[1]
          }
        })
      end
    "
    remove_field => "data_cleaner_info"
    add_field => { "warehouse" => "{{ Indicia warehouse unique name }}" }
  }
  # Convert JSON text to JSON
  json {
    source => "sample_attrs_json"
    target => "sample_attrs_orig"
    remove_field => "sample_attrs_json"
  }
  json {
    source => "occurrence_attrs_json"
    target => "occurrence_attrs_orig"
    remove_field => "occurrence_attrs_json"
  }
  ruby {
    code => "
    require 'uri'
    ['sample', 'occurrence'].each do |source|
      if event.get(source + '_attrs_orig')
        attrs = []
        event.get(source + '_attrs').to_hash.each do |key,value|
          if not value =~ URI::MailTo::EMAIL_REGEXP
            attrObj = {}
            attrObj['id'] = key
            attrObj['value'] = value
            attrs.push(attrObj)
          end
        end
        event.set(source + '_attrs', attrs)
      end
    end
    "
    remove_field => "sample_attrs_orig"
    remove_field => "occurrence_attrs_orig"
  }
  mutate {
    rename => {
      "created_on" => "[metadata][created_on]"
      "updated_on" => "[metadata][updated_on]"
      "created_by_id" => "[metadata][created_by_id]"
      "updated_by_id" => "[metadata][updated_by_id]"
      "licence_code" => "[metadata][licence_code]"
      "group_id" => "[metadata][group][id]"
      "group_title" => "[metadata][group][title]"
      "survey_id" => "[metadata][survey][id]"
      "survey_title" => "[metadata][survey][title]"
      "website_id" => "[metadata][website][id]"
      "website_title" => "[metadata][website][title]"
      "sensitive" => "[metadata][sensitive]"
      "sensitivity_precision" => "[metadata][sensitivity_precision]"
      "sensitivity_blur" => "[metadata][sensitivity_blur]"
      "trial" => "[metadata][trial]"
      "confidential" => "[metadata][confidential]"
      "release_status" => "[metadata][release_status]"
      "tracking" => "[metadata][tracking]"
      "input_form" => "[metadata][input_form]"
      "attr_det_name" => "[identification][identified_by]"
      "verifier" => "[identification][verifier][name]"
      "verified_by_id" => "[identification][verifier][id]"
      "verified_on" => "[identification][verified_on]"
      "record_status" => "[identification][verification_status]"
      "record_substatus" => "[identification][verification_substatus]"
      "record_decision_source" => "[identification][verification_decision_source]"
      "data_cleaner_list" => "[identification][auto_checks][output]"
      "verification_checks_enabled" => "[identification][auto_checks][enabled]"
      "query" => "[identification][query]"
      "attr_certainty" => "[identification][recorder_certainty]"
      "point" => "[location][point]"
      "geom" => "[location][geom]"
      "map_sq_srid" => "[location][grid_square][srid]"
      "map_sq_1km" => "[location][grid_square][1km][centre]"
      "map_sq_2km" => "[location][grid_square][2km][centre]"
      "map_sq_10km" => "[location][grid_square][10km][centre]"
      "input_sref" => "[location][input_sref]"
      "input_sref_system" => "[location][input_sref_system]"
      "output_sref" => "[location][output_sref]"
      "output_sref_system" => "[location][output_sref_system]"
      "coordinate_uncertainty_in_meters" => "[location][coordinate_uncertainty_in_meters]"
      "given_locality_name" => "[location][verbatim_locality]"
      "recorded_location_id" => "[location][location_id]"
      "recorded_location_name" => "[location][name]"
      "recorded_parent_location_id" => "[location][parent][location_id]"
      "recorded_parent_location_name" => "[location][parent][name]"
      "indexed_location_ids" => "[location][higher_geography_ids]"
      "sample_id" => "[event][event_id]"
      "parent_sample_id" => "[event][parent_event_id]"
      "date_start" => "[event][date_start]"
      "date_end" => "[event][date_end]"
      "day_of_year" => "[event][day_of_year]"
      "week" => "[event][week]"
      "ukbms_week" => "[event][ukbms_week]"
      "month" => "[event][month]"
      "year" => "[event][year]"
      "recorders" => "[event][recorded_by]"
      "sample_comment" => "[event][event_remarks]"
      "attr_biotope" => "[event][habitat]"
      "attr_sample_method" => "[event][sampling_protocol]"
      "sample_attrs" => "[event][attributes]"
      "taxon_key" => "[taxon][taxon_id]"
      "taxa_taxon_list_id" => "[taxon][taxa_taxon_list_id]"
      "taxon_meaning_id" => "[taxon][taxon_meaning_id]"
      "taxon_list_id" => "[taxon][taxon_list][id]"
      "taxon_list_title" => "[taxon][taxon_list][title]"
      "taxon_name" => "[taxon][taxon_name]"
      "taxon_authority" => "[taxon][taxon_name_authorship]"
      "accepted_taxon_key" => "[taxon][accepted_taxon_id]"
      "accepted_taxon_name" => "[taxon][accepted_name]"
      "accepted_taxon_authority" => "[taxon][accepted_name_authorship]"
      "taxon_group_id" => "[taxon][group_id]"
      "taxon_group" => "[taxon][group]"
      "common_name" => "[taxon][vernacular_name]"
      "taxon_rank" => "[taxon][taxon_rank]"
      "taxon_rank_sort_order" => "[taxon][taxon_rank_sort_order]"
      "marine_flag" => "[taxon][marine]"
      "freshwater_flag" => "[taxon][freshwater]"
      "terrestrial_flag" => "[taxon][terrestrial]"
      "non_native_flag" => "[taxon][non_native]"
      "taxon_kingdom" => "[taxon][kingdom]"
      "taxon_phylum" => "[taxon][phylum]"
      "taxon_class" => "[taxon][class]"
      "taxon_order" => "[taxon][order]"
      "taxon_family" => "[taxon][family]"
      "taxon_subfamily" => "[taxon][subfamily]"
      "taxon_genus" => "[taxon][genus]"
      "taxon_species" => "[taxon][species]"
      "taxon_species_key" => "[taxon][species_taxon_id]"
      "taxon_path_keys" => "[taxon][higher_taxon_ids]"
      "external_key" => "[occurrence][source_system_key]"
      "attr_sex" => "[occurrence][sex]"
      "attr_stage" => "[occurrence][life_stage]"
      "attr_sex_stage_count" => "[occurrence][organism_quantity]"
      "attr_sex_stage_count_exact" => "[occurrence][individual_count]"
      "zero_abundance" => "[occurrence][zero_abundance]"
      "comment" => "[occurrence][occurrence_remarks]"
      "media_list" => "[occurrence][media]"
      "occurrence_attrs" => "[occurrence][attributes]"
    }
  }
  ruby {
    # Script to clean up nulls and empty values.
    path => "{{ Working folder path }}/logstash-rb/compact_event.rb"
  }
}
output {
  if [id] {
    elasticsearch {
      hosts => ["{{ Elasticsearch address }}"]
      index => "occurrence_{{ Indicia warehouse unique name }}_index"
      # Our records need a unique ID in the index for updates.
      document_id => "{{ Indicia warehouse unique name }}|%{id}"
    }
    # Uncomment the following to see the normal events output to the terminal.
    # stdout { codec => json }
  }
  else {
    elasticsearch {
      hosts => ["{{ Elasticsearch address }}"]
      index => "occurrence_{{ Indicia warehouse unique name }}_errors"
    }
    # Uncomment the following to see the error events output to the terminal.
    # stdout { codec => json }
  }
}
